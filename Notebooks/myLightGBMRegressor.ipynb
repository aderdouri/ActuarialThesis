{"cells":[{"cell_type":"markdown","metadata":{"id":"7hq02MAexQov"},"source":["# Regression LightGBM"]},{"cell_type":"markdown","source":["## Cloner la branche contenant le dateset le le code qui va avec."],"metadata":{"id":"eaB5K5Vnv_Zr"}},{"cell_type":"code","source":["!rm -rf ActuarialThesis\n","!git clone https://github.com/aderdouri/ActuarialThesis.git\n","%ls -ltr ActuarialThesis"],"metadata":{"id":"MIzuG9lLv-qB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir ActuarialThesis/plots_rgr\n","!ls -ltr ActuarialThesis/plots_rgr"],"metadata":{"id":"_rUJ_QVHwJ6h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ajouter le répértoire src\n","import sys\n","sys.path.insert(0,'./ActuarialThesis/src/')"],"metadata":{"id":"WNxNTNl3wPGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import installHelper"],"metadata":{"id":"gQK3czwfwQn0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(list(dir(installHelper)))"],"metadata":{"id":"na4NiGkKwSA6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls -ltr"],"metadata":{"id":"Dq4LUfuawTqZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Installer les packages nécéssaires"],"metadata":{"id":"afsLDKQ0wI8A"}},{"cell_type":"code","source":["installHelper.installALL()"],"metadata":{"id":"ZsHliYDgwYKP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# On doit trouver tous les packages mentionés dans le grep\n","!pip list -v | grep -e catboost -e 'imbalanced-learn' -e 'optuna' -e 'catboost' -e 'lime' -e 'shap'"],"metadata":{"id":"jPdM42hywbNJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Importer les packages nécéssaires"],"metadata":{"id":"0O3HrtEjwc5N"}},{"cell_type":"code","source":["from helper import *"],"metadata":{"id":"NwMKtyB7wgPm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Appliquer le theme par défaut\n","sns.set_theme()"],"metadata":{"id":"WWAzE3wBwiOs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Partir du dataset déjé encodé."],"metadata":{"id":"l3Nm2HxiwfqU"}},{"cell_type":"code","source":["# Partir du dataset déja encodé.\n","df = pd.read_csv('ActuarialThesis/Data/encodedBASEAUTO.csv')\n","df.head()"],"metadata":{"id":"ZbHLYQArwqKZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = df.drop('CHARGE', axis=1)\n","y = df['CHARGE']"],"metadata":{"id":"WaIlOIpqzn2x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tout d'abord, nous séparons la cible du cadre de données avec des caractéristiques (df -> X, y).\n","\n","Ensuite, nous divisons les données en ensembles train/val/test dans le rapport 60:20:20. L'idée est que nous utiliserons l'ensemble train pour entraîner nos modèles, l'ensemble val pour les valider et l'ensemble test pour calculer l'erreur finale du mélange. L'ensemble de test sera donc constitué de données totalement inédites.\n","\n","Pour ce faire, utilisez un train_test_split régulier de sklearn pour diviser X et y en parties train et val/test dans le ratio 60:40. Ensuite, utilisez à nouveau train_test_split, mais pour diviser la partie val/test obtenue en validation et test dans un rapport 50:50. Dans chaque application de train_test_split, utilisez random_state=13 et les autres valeurs de paramètres par défaut.\n","\n","Au final, vous devriez obtenir X_train, X_val, X_test avec les formes suivantes, respectivement : (23786, 58), (7929, 58), (7929, 58). La même logique s'applique à y_train, y_val, y_test."],"metadata":{"id":"IN1fZnjrzZRL"}},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=13)\n","X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=13)"],"metadata":{"id":"oFixPQiNzPfO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from lightgbm import LGBMRegressor\n","from lightgbm import early_stopping, log_evaluation\n","from sklearn.metrics import mean_squared_error"],"metadata":{"id":"4XZs7RFdzF6k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lgbm_rgr = LGBMRegressor(objective='regression', \n","                         n_estimators=200,\n","                         learning_rate=0.01, \n","                         max_depth=5, \n","                         random_state=13)\n","\n","lgbm_rgr.fit(X_train, y_train, \n","             eval_set=[(X_val, y_val)], \n","             eval_metric='rmse', \n","             callbacks=[early_stopping(stopping_rounds=50),\n","             log_evaluation(period=20, show_stdv=True)\n","             ]\n","        )\n","y_pred_lgbm_rgr = lgbm_rgr.predict(X_val)\n","rmse = mean_squared_error(y_val, y_pred_lgbm_rgr, squared=False)\n","print(\"RMSE: %.5f\" % rmse)"],"metadata":{"id":"Kq_NhW2J0Jr-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create two subplots and unpack the output array immediately\n","fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True,  figsize=(15, 4))\n","\n","# Create scatter plot with actual and predicted values\n","sns.scatterplot(ax=ax1, x=y_val, y=y_pred_lgbm_rgr)\n","ax1.set_xlabel('Actual Values')\n","ax1.set_ylabel('Predicted Values')\n","ax1.set_title('Actual vs Predicted Values')\n","\n","# Create regression plot with actual and predicted values\n","sns.regplot(ax=ax2, x=y_val, y=y_pred_lgbm_rgr, scatter_kws={'s': 10}, line_kws={'color': 'red'})\n","ax2.set_xlabel('Predicted Values')\n","ax2.set_ylabel('Residuals')\n","ax2.set_title('Residual Plot of Actual vs Predicted Values');"],"metadata":{"id":"pub4_rIAhOoL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cross_validation_fcn(X_train, model, early_stopping_flag=False):\n","    \"\"\"\n","    Performs cross-validation on a given model using KFold and returns the average\n","    mean squared error (MSE) score across all folds.\n","\n","    Parameters:\n","    - X_train: the training data to use for cross-validation\n","    - model: the machine learning model to use for cross-validation\n","    - early_stopping_flag: a boolean flag to indicate whether early stopping should be used\n","\n","    Returns:\n","    - model: the trained machine learning model\n","    - mean_mse: the average MSE score across all folds\n","    \"\"\"\n","    mse_list = []\n","    for train_index, val_index in kf.split(X_train):\n","        # Split the data into training and validation sets\n","        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n","        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n","        \n","        # Train the model on the training set\n","        if early_stopping_flag:\n","            # Use early stopping if enabled\n","            model.fit(X_train_fold, y_train_fold, \n","                      eval_set=[(X_val_fold, y_val_fold)],\n","                      callbacks=[early_stopping(stopping_rounds=250, verbose=0)])\n","        else:\n","            model.fit(X_train_fold, y_train_fold)\n","            \n","        # Make predictions on the validation set and calculate the MSE score\n","        y_pred = model.predict(X_val_fold)\n","        mse = mean_squared_error(y_val_fold, y_pred, squared=False)\n","        mse_list.append(mse)\n","        \n","    # Return the trained model and the average MSE score\n","    return model, np.mean(mse_list)"],"metadata":{"id":"uAJX0bspFU5z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed = 2042\n","from sklearn.model_selection import KFold\n","n_folds = 5\n","# create KFold object\n","kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n","from sklearn.metrics import r2_score, accuracy_score"],"metadata":{"id":"k9koNwyMFsT4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define the objective function for Optuna optimization\n","def objective(trial):\n","    # set up the parameters to be optimized\n","    param = {\n","        'objective': trial.suggest_categorical('objective', ['regression', 'tweedie']),\n","        'metric': trial.suggest_categorical('metric', ['rmse']),\n","        'random_state': trial.suggest_categorical('random_state', [seed]),\n","        'n_estimators': trial.suggest_categorical('n_estimators', [10000]),\n","        'reg_alpha': trial.suggest_float('reg_alpha', 1e-4, 10.0,log=True),\n","        'reg_lambda': trial.suggest_float('reg_lambda', 1e-4, 10.0,log=True),\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n","        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n","        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2,log=True),\n","        'max_depth': trial.suggest_int('max_depth', 4, 8),\n","        'num_leaves': trial.suggest_int('num_leaves', 40, 100),\n","        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n","        'cat_smooth': trial.suggest_int('cat_smooth', 1, 100),\n","        'force_col_wise': trial.suggest_categorical('force_col_wise', [True])      \n","    }\n","\n","    if param[\"objective\"] == \"tweedie\":\n","        param[\"tweedie_variance_power\"] = trial.suggest_float(\"tweedie_variance_power\", 1.1, 1.9)\n","\n","\n","    # create the LightGBM regressor with the optimized parameters\n","    model = LGBMRegressor(**param)\n","    \n","    # perform cross-validation using the optimized LightGBM regressor\n","    model, mean_score =  cross_validation_fcn(X_train, \n","                                              model, \n","                                              early_stopping_flag=True)\n","        \n","    # retrieve the best iteration of the model and store it as a user attribute in the trial object\n","    best_iteration = model.best_iteration_\n","    trial.set_user_attr('best_iteration', best_iteration)\n","        \n","    return mean_score"],"metadata":{"id":"kEoMCsFa94EU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create an optimization study with Optuna library\n","study = optuna.create_study(direction=\"minimize\",study_name=\"lgbm_opt\")\n","# Optimize the study using a user-defined objective function, for a total of 50 trials\n","study.optimize(objective, n_trials=3)"],"metadata":{"id":"Mn3-1OVdFi8e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Number of finished trials: \", len(study.trials))\n","print(\"Best trial:\")\n","trial = study.best_trial\n","hp_lgbm = study.best_params\n","hp_lgbm[\"n_estimators\"] = study.best_trial.user_attrs['best_iteration']\n","\n","# Print the objective value and the set of hyperparameters of the best trial\n","print(\"  Value: {}\".format(trial.value))\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","    print(\"    {}: {}\".format(key, value))"],"metadata":{"id":"JzGe-iGQFmFE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = optuna.visualization.plot_param_importances(study)\n","fig.write_image(\"./ActuarialThesis/plots_rgr/myLightGBMRegressorParamImportances.pdf\")\n","fig.show()"],"metadata":{"id":"WUHoqkZc57P-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimized_lgbm_rgr = LGBMRegressor(**hp_lgbm)\n","optimized_lgbm_rgr.fit(X_train, y_train)\n","y_pred_lgbm_rgr = optimized_lgbm_rgr.predict(X_val)\n","\n","print(\"Best rmse:\", mean_squared_error(y_pred_lgbm_rgr, y_val, squared=False))\n","print(\"R2 using LightGBM: \", r2_score(y_val, y_pred_lgbm_rgr ))"],"metadata":{"id":"CbtByQ2h10Wx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reduced_features = ['AGECOND', 'RM', 'ACV']\n","X_train_reduced = X_train[reduced_features]\n","X_val_reduced = X_val[reduced_features]\n","\n","optimized_lgbm_rgr = LGBMRegressor(**hp_lgbm)\n","optimized_lgbm_rgr.fit(X_train_reduced, y_train)\n","y_pred_lgbm_rgr = optimized_lgbm_rgr.predict(X_val_reduced)\n","\n","print(\"Best rmse:\", mean_squared_error(y_pred_lgbm_rgr, y_val, squared=False))\n","print(\"R2 using LightGBM: \", r2_score(y_val, y_pred_lgbm_rgr ))"],"metadata":{"id":"8Rzg2Lifbya7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(X_train)"],"metadata":{"id":"5KHP0J8gcYEf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from lightgbm import plot_importance\n","ax = plot_importance(optimized_lgbm_rgr, max_num_features=10)\n","ax.figure.set_size_inches(20, 6)\n","ax.figure.savefig('./ActuarialThesis/plots_rgr/myLightGBMRegressorFeatureImportance.pdf')"],"metadata":{"id":"FHHIx7S66WlJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create two subplots and unpack the output array immediately\n","fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True,  figsize=(15, 4))\n","\n","# Create scatter plot with actual and predicted values\n","sns.scatterplot(ax=ax1, x=y_test, y=y_pred_lgbm_rgr)\n","ax1.set_xlabel('Actual Values')\n","ax1.set_ylabel('Predicted Values')\n","ax1.set_title('Actual vs Predicted Values')\n","\n","# Create regression plot with actual and predicted values\n","sns.regplot(ax=ax2, x=y_test, y=y_pred_lgbm_rgr, scatter_kws={'s': 10}, line_kws={'color': 'red'})\n","ax2.set_xlabel('Predicted Values')\n","ax2.set_ylabel('Residuals')\n","ax2.set_title('Residual Plot of Actual vs Predicted Values');\n","\n","fig.savefig('./ActuarialThesis/plots_rgr/myLightGBMRegressorActualvsPredicted.pdf')"],"metadata":{"id":"AYzkU5mw6pXC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#filename = \"optimized_xgb_rgr.pkl\"\n","#pickle.dump(optimized_xgb_rgr, open(filename, \"wb\"))"],"metadata":{"id":"Ci7Oy_P249o6"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","private_outputs":true},"coursera":{"schema_names":["ensembling-techniques-task-week-2"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}